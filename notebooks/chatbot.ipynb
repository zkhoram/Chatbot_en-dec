{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FCsVR_UDv0-"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEzLjp3qDv1D"
      },
      "source": [
        "Preparations\n",
        "------------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EFxJ4PJDv1G"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import unicode_literals\n",
        "\n",
        "import torch\n",
        "from torch.jit import script, trace\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import csv\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "import unicodedata\n",
        "import codecs\n",
        "from io import open\n",
        "import itertools\n",
        "import math\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data cleaning"
      ],
      "metadata": {
        "id": "NvvLiOfTQD2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hazm"
      ],
      "metadata": {
        "id": "PcVKAx4pPkMI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6c74841-70f1-4f62-a052-a8d8355bad0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting hazm\n",
            "  Downloading hazm-0.7.0-py3-none-any.whl (316 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.7/316.7 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nltk==3.3\n",
            "  Downloading nltk-3.3.0.zip (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting libwapiti>=0.2.1\n",
            "  Downloading libwapiti-0.2.1.tar.gz (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from nltk==3.3->hazm) (1.16.0)\n",
            "Building wheels for collected packages: nltk, libwapiti\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394489 sha256=30e51dc0b59b001d3ce798a89b4a3e3a72ec12c82821346e406ac5b485fcfb3a\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/6d/14/3defa4cd7013faeddf715150696f4a96d7725c87700eb8a68e\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp310-cp310-linux_x86_64.whl size=180376 sha256=27184d9f08f1ba688047a6d5fb5b450e0052babea086908259d5960335ee3635\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/cb/30/fef48ecac051e433987eccdb5682900b4c00d44a4bcd4d4ec8\n",
            "Successfully built nltk libwapiti\n",
            "Installing collected packages: nltk, libwapiti, hazm\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.8.1\n",
            "    Uninstalling nltk-3.8.1:\n",
            "      Successfully uninstalled nltk-3.8.1\n",
            "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from __future__ import unicode_literals\n",
        "from hazm import *\n",
        "import re"
      ],
      "metadata": {
        "id": "UHDI_VSTPpYe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "755001fd-f0be-41b5-bb59-85bc3a8e38b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir='rtl'>\n",
        "حذف حروف انگلیسی\n",
        "\n",
        "حذف ایموجی ها\n",
        "\n",
        "حذف علایم نگارشی تکرار شده مثل ....\n",
        "\n",
        "حذف حروف تکرار شونده بیشتر از سه بار\n",
        "\n",
        "نرمالایز کردن متن\n"
      ],
      "metadata": {
        "id": "--zXsjiZP2mo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalizer = Normalizer()\n",
        "def process(full_text):\n",
        "    cleaned_text = clean_tweet(text=full_text)\n",
        "    normalized_text = normalizer.normalize(cleaned_text)\n",
        "    normalized_text = re.sub('\\u200c',' ',normalized_text)#???\n",
        "    tokenized_text = tokenizer_hazm(normalized_text)\n",
        "    return \" \".join(tokenized_text)\n",
        "\n",
        "def clean_tweet(text=None):\n",
        "    \"\"\" get text of tweet and clean it.\n",
        "        cleaning is done for:\n",
        "        1- urls\n",
        "        2- emoticons\n",
        "        3- mentions from the beginning of text\n",
        "        4- remove hashtags from the beginning and end of text\n",
        "        \n",
        "        5- delete english charachters\n",
        "        6- delete repeated punctuations \n",
        "        7- remove repeated chatacter that repeate more than 3 times\n",
        "\n",
        "    :param text: fulltext of tweet\n",
        "    :return: cleaned text\n",
        "    \"\"\"\n",
        "\n",
        "    full_text = text\n",
        "\n",
        "    # remove urls\n",
        "    regex = re.compile('https:\\S*')\n",
        "    it = re.finditer(regex, full_text)\n",
        "    for match in it:\n",
        "        full_text = full_text.replace(match.group(0), \"\")\n",
        "\n",
        "    # remove emoticons\n",
        "    emoji_pattern = re.compile(u'['\n",
        "                               u'\\U0001F300-\\U0001F64F'\n",
        "                               u'\\U0001F680-\\U0001F6FF'\n",
        "                               u'\\u2600-\\u26FF\\u2700-\\u27BF]+',\n",
        "                               re.UNICODE)\n",
        "    full_text = emoji_pattern.sub('', full_text)\n",
        "\n",
        "    # remove mentions\n",
        "    regex = re.compile('@\\w*')\n",
        "    it = re.finditer(regex, full_text)\n",
        "    for match in it:\n",
        "        full_text = full_text.replace(match.group(0), \"\")\n",
        "\n",
        "    # remove mention sign from rest of text\n",
        "    full_text = full_text.replace(\"@\", \"\")\n",
        "\n",
        "    # remove hashtags at the end of string\n",
        "    regex = re.compile('([\\n\\s]*#\\w*)*\\Z')\n",
        "    it = re.finditer(regex, full_text)\n",
        "    for match in it:\n",
        "        full_text = full_text.replace(match.group(0), \"\")\n",
        "\n",
        "    # remove hashtags in the begining of string\n",
        "    regex = re.compile('\\A([\\s\\n]*#\\w*\\s*\\n+\\s*)*')\n",
        "    it = re.finditer(regex, full_text)\n",
        "    for match in it:\n",
        "        full_text = full_text.replace(match.group(0), \"\")\n",
        "\n",
        "    full_text = full_text.replace(\"#\", \"\")\n",
        "    full_text = full_text.replace(\"RT\", \"\")\n",
        "\n",
        "    # delete english charachters\n",
        "    full_text = re.sub('[^\\u0600-\\u06FF\\s]','',full_text)\n",
        "\n",
        "    # delete repeated punctuations\n",
        "    rx = re.compile(r'([!-/:-@[-`{-~])\\1+')\n",
        "    full_text = rx.sub(r'\\1 ', full_text)\n",
        "\n",
        "    #convert half space to space\n",
        "    #? hazm added ?\n",
        "    full_text =  re.sub('\\u200c',' ',full_text)\n",
        "    # remove repeated chatacter that repeate more than 3 times\n",
        "    #?\n",
        "\n",
        "    return full_text.strip()\n",
        "\n",
        "\n",
        "def tokenizer_hazm(text):\n",
        "    tokenized = []\n",
        "    for sent in sent_tokenize(text):\n",
        "        tokenized.extend(word_tokenize(sent))\n",
        "    return tokenized"
      ],
      "metadata": {
        "id": "Bmh_vTjgPgOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Format Dataset"
      ],
      "metadata": {
        "id": "j2fbv8aKHvEN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PerCQA\n"
      ],
      "metadata": {
        "id": "YeiSfKtS2lxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_p = pd.read_csv('PerCQA_CSV_Format.csv',error_bad_lines = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmyilPVZHx2R",
        "outputId": "df5b1f62-7da4-4dac-db38-aa7c7d62425e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-31cd85f38b6c>:1: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  df_p = pd.read_csv('PerCQA_CSV_Format.csv',error_bad_lines = False)\n",
            "Skipping line 652: expected 10 fields, saw 12\n",
            "Skipping line 1231: expected 10 fields, saw 11\n",
            "Skipping line 1747: expected 10 fields, saw 11\n",
            "Skipping line 1781: expected 10 fields, saw 19\n",
            "Skipping line 2519: expected 10 fields, saw 12\n",
            "Skipping line 2523: expected 10 fields, saw 11\n",
            "Skipping line 2747: expected 10 fields, saw 11\n",
            "Skipping line 2953: expected 10 fields, saw 11\n",
            "Skipping line 2958: expected 10 fields, saw 12\n",
            "Skipping line 2959: expected 10 fields, saw 11\n",
            "Skipping line 3189: expected 10 fields, saw 11\n",
            "Skipping line 3204: expected 10 fields, saw 14\n",
            "Skipping line 3206: expected 10 fields, saw 11\n",
            "Skipping line 3242: expected 10 fields, saw 12\n",
            "Skipping line 3554: expected 10 fields, saw 11\n",
            "Skipping line 3674: expected 10 fields, saw 11\n",
            "Skipping line 4371: expected 10 fields, saw 11\n",
            "Skipping line 4375: expected 10 fields, saw 11\n",
            "Skipping line 4630: expected 10 fields, saw 11\n",
            "Skipping line 4634: expected 10 fields, saw 11\n",
            "Skipping line 4637: expected 10 fields, saw 11\n",
            "Skipping line 4958: expected 10 fields, saw 11\n",
            "Skipping line 4986: expected 10 fields, saw 12\n",
            "Skipping line 5348: expected 10 fields, saw 11\n",
            "Skipping line 5416: expected 10 fields, saw 11\n",
            "Skipping line 6023: expected 10 fields, saw 12\n",
            "Skipping line 6071: expected 10 fields, saw 15\n",
            "Skipping line 6333: expected 10 fields, saw 11\n",
            "Skipping line 7002: expected 10 fields, saw 11\n",
            "Skipping line 7007: expected 10 fields, saw 11\n",
            "Skipping line 7027: expected 10 fields, saw 11\n",
            "Skipping line 7052: expected 10 fields, saw 14\n",
            "Skipping line 7086: expected 10 fields, saw 11\n",
            "Skipping line 7100: expected 10 fields, saw 11\n",
            "Skipping line 7108: expected 10 fields, saw 11\n",
            "Skipping line 7149: expected 10 fields, saw 11\n",
            "Skipping line 7346: expected 10 fields, saw 11\n",
            "Skipping line 7843: expected 10 fields, saw 13\n",
            "Skipping line 8321: expected 10 fields, saw 12\n",
            "Skipping line 8473: expected 10 fields, saw 11\n",
            "Skipping line 8494: expected 10 fields, saw 11\n",
            "Skipping line 8546: expected 10 fields, saw 11\n",
            "Skipping line 9493: expected 10 fields, saw 11\n",
            "Skipping line 9498: expected 10 fields, saw 14\n",
            "Skipping line 9504: expected 10 fields, saw 12\n",
            "Skipping line 10412: expected 10 fields, saw 14\n",
            "Skipping line 10414: expected 10 fields, saw 11\n",
            "Skipping line 10417: expected 10 fields, saw 14\n",
            "Skipping line 10651: expected 10 fields, saw 11\n",
            "Skipping line 11082: expected 10 fields, saw 12\n",
            "Skipping line 11087: expected 10 fields, saw 13\n",
            "Skipping line 11259: expected 10 fields, saw 11\n",
            "Skipping line 11275: expected 10 fields, saw 15\n",
            "Skipping line 11281: expected 10 fields, saw 11\n",
            "Skipping line 11525: expected 10 fields, saw 11\n",
            "Skipping line 12298: expected 10 fields, saw 11\n",
            "Skipping line 12818: expected 10 fields, saw 15\n",
            "Skipping line 12820: expected 10 fields, saw 11\n",
            "Skipping line 12828: expected 10 fields, saw 12\n",
            "Skipping line 12837: expected 10 fields, saw 11\n",
            "Skipping line 12982: expected 10 fields, saw 11\n",
            "Skipping line 13193: expected 10 fields, saw 11\n",
            "Skipping line 13194: expected 10 fields, saw 11\n",
            "Skipping line 13195: expected 10 fields, saw 11\n",
            "Skipping line 13196: expected 10 fields, saw 11\n",
            "Skipping line 13197: expected 10 fields, saw 11\n",
            "Skipping line 13198: expected 10 fields, saw 11\n",
            "Skipping line 13199: expected 10 fields, saw 11\n",
            "Skipping line 13475: expected 10 fields, saw 11\n",
            "Skipping line 13813: expected 10 fields, saw 11\n",
            "Skipping line 13958: expected 10 fields, saw 12\n",
            "Skipping line 13987: expected 10 fields, saw 12\n",
            "Skipping line 13993: expected 10 fields, saw 14\n",
            "Skipping line 13995: expected 10 fields, saw 11\n",
            "Skipping line 14269: expected 10 fields, saw 11\n",
            "Skipping line 15624: expected 10 fields, saw 11\n",
            "Skipping line 16002: expected 10 fields, saw 11\n",
            "Skipping line 16285: expected 10 fields, saw 11\n",
            "Skipping line 17499: expected 10 fields, saw 11\n",
            "Skipping line 17763: expected 10 fields, saw 11\n",
            "Skipping line 18800: expected 10 fields, saw 11\n",
            "Skipping line 19152: expected 10 fields, saw 14\n",
            "Skipping line 19408: expected 10 fields, saw 12\n",
            "Skipping line 19647: expected 10 fields, saw 11\n",
            "Skipping line 19784: expected 10 fields, saw 11\n",
            "Skipping line 19832: expected 10 fields, saw 11\n",
            "Skipping line 20335: expected 10 fields, saw 11\n",
            "Skipping line 20403: expected 10 fields, saw 11\n",
            "Skipping line 20945: expected 10 fields, saw 11\n",
            "Skipping line 21218: expected 10 fields, saw 13\n",
            "Skipping line 21319: expected 10 fields, saw 11\n",
            "Skipping line 21869: expected 10 fields, saw 11\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(df_p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fK3nah94JeGV",
        "outputId": "140e7107-9314-4f3e-f9c4-3279dabc80c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21999"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_p.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "id": "JquJVhA8JoTP",
        "outputId": "ea78a695-9731-4733-c035-732d96799e7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       QID QUsername             QDATE         QSubject  \\\n",
              "0  3270053    سوشااا  2019/09/21 12:50  قد دختر سه ساله   \n",
              "1  3270053    سوشااا  2019/09/21 12:50  قد دختر سه ساله   \n",
              "2  3270053    سوشااا  2019/09/21 12:50  قد دختر سه ساله   \n",
              "3  3270053    سوشااا  2019/09/21 12:50  قد دختر سه ساله   \n",
              "4  3270053    سوشااا  2019/09/21 12:50  قد دختر سه ساله   \n",
              "\n",
              "                                       QBody        CID CUSERID  \\\n",
              "0  دخترم۳سالو نیمه قدش ۹۵ وزنش۱۴ خوبه یا کمه  105626741   32122   \n",
              "1  دخترم۳سالو نیمه قدش ۹۵ وزنش۱۴ خوبه یا کمه  105626772    1884   \n",
              "2  دخترم۳سالو نیمه قدش ۹۵ وزنش۱۴ خوبه یا کمه  105626911   39292   \n",
              "3  دخترم۳سالو نیمه قدش ۹۵ وزنش۱۴ خوبه یا کمه  105626962   33018   \n",
              "4  دخترم۳سالو نیمه قدش ۹۵ وزنش۱۴ خوبه یا کمه  105627088    3449   \n",
              "\n",
              "       CUsername                                         CBodyClean CGOLD  \n",
              "0       خوششانسم  پسرم یه ماه دیگه میشه سه سال.قدش حدودا دو ماه ...  Good  \n",
              "1           مربا  نمیدونم پسرم 25 ماهشه قدش 95. خودتون قدتون چجو...  Good  \n",
              "2  asalbanojonam  من پسرم دوسال ونه ماهش هس قد93وزن 13ترخدا بگید...  Good  \n",
              "3      متولدماه7  عزیزم بچه هارو مقایسه نکن .حتی خواهر بردار ها ...  Good  \n",
              "4       Miss.iDa  دختر منم 3 سالشه قدش یه ماه پیش 89 بود وزنش سی...  Good  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0b9ceaee-3f63-4c00-9df5-91297050bc0d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>QID</th>\n",
              "      <th>QUsername</th>\n",
              "      <th>QDATE</th>\n",
              "      <th>QSubject</th>\n",
              "      <th>QBody</th>\n",
              "      <th>CID</th>\n",
              "      <th>CUSERID</th>\n",
              "      <th>CUsername</th>\n",
              "      <th>CBodyClean</th>\n",
              "      <th>CGOLD</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3270053</td>\n",
              "      <td>سوشااا</td>\n",
              "      <td>2019/09/21 12:50</td>\n",
              "      <td>قد دختر سه ساله</td>\n",
              "      <td>دخترم۳سالو نیمه قدش ۹۵ وزنش۱۴ خوبه یا کمه</td>\n",
              "      <td>105626741</td>\n",
              "      <td>32122</td>\n",
              "      <td>خوششانسم</td>\n",
              "      <td>پسرم یه ماه دیگه میشه سه سال.قدش حدودا دو ماه ...</td>\n",
              "      <td>Good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3270053</td>\n",
              "      <td>سوشااا</td>\n",
              "      <td>2019/09/21 12:50</td>\n",
              "      <td>قد دختر سه ساله</td>\n",
              "      <td>دخترم۳سالو نیمه قدش ۹۵ وزنش۱۴ خوبه یا کمه</td>\n",
              "      <td>105626772</td>\n",
              "      <td>1884</td>\n",
              "      <td>مربا</td>\n",
              "      <td>نمیدونم پسرم 25 ماهشه قدش 95. خودتون قدتون چجو...</td>\n",
              "      <td>Good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3270053</td>\n",
              "      <td>سوشااا</td>\n",
              "      <td>2019/09/21 12:50</td>\n",
              "      <td>قد دختر سه ساله</td>\n",
              "      <td>دخترم۳سالو نیمه قدش ۹۵ وزنش۱۴ خوبه یا کمه</td>\n",
              "      <td>105626911</td>\n",
              "      <td>39292</td>\n",
              "      <td>asalbanojonam</td>\n",
              "      <td>من پسرم دوسال ونه ماهش هس قد93وزن 13ترخدا بگید...</td>\n",
              "      <td>Good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3270053</td>\n",
              "      <td>سوشااا</td>\n",
              "      <td>2019/09/21 12:50</td>\n",
              "      <td>قد دختر سه ساله</td>\n",
              "      <td>دخترم۳سالو نیمه قدش ۹۵ وزنش۱۴ خوبه یا کمه</td>\n",
              "      <td>105626962</td>\n",
              "      <td>33018</td>\n",
              "      <td>متولدماه7</td>\n",
              "      <td>عزیزم بچه هارو مقایسه نکن .حتی خواهر بردار ها ...</td>\n",
              "      <td>Good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3270053</td>\n",
              "      <td>سوشااا</td>\n",
              "      <td>2019/09/21 12:50</td>\n",
              "      <td>قد دختر سه ساله</td>\n",
              "      <td>دخترم۳سالو نیمه قدش ۹۵ وزنش۱۴ خوبه یا کمه</td>\n",
              "      <td>105627088</td>\n",
              "      <td>3449</td>\n",
              "      <td>Miss.iDa</td>\n",
              "      <td>دختر منم 3 سالشه قدش یه ماه پیش 89 بود وزنش سی...</td>\n",
              "      <td>Good</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0b9ceaee-3f63-4c00-9df5-91297050bc0d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0b9ceaee-3f63-4c00-9df5-91297050bc0d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0b9ceaee-3f63-4c00-9df5-91297050bc0d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_p = df_p [['QBody','CBodyClean']]"
      ],
      "metadata": {
        "id": "Pio3ny2rJvlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.savetxt(r'PerCQA_pre_formatted.txt', df_p.values, fmt='%s', delimiter='\\t')"
      ],
      "metadata": {
        "id": "Uyuu7LaA4tvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines_p = open('PerCQA_pre_formatted.txt', encoding='utf-8').read().strip().split('\\n')\n",
        "pairs_p = [[process(s) for s in l.split('\\t')] for l in lines_p]"
      ],
      "metadata": {
        "id": "a2cf_5x84-jZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1_p = pd.DataFrame(pairs_p)"
      ],
      "metadata": {
        "id": "BrODiDMO51m4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1_p.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "xtNisyUN56MW",
        "outputId": "5c07e5cb-d497-484f-a20c-38a613b7cb57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            0  \\\n",
              "0  دخترم۳سالو نیمه قدش ۹۵ وزنش ۱۴ خوبه یا کمه   \n",
              "1  دخترم۳سالو نیمه قدش ۹۵ وزنش ۱۴ خوبه یا کمه   \n",
              "2  دخترم۳سالو نیمه قدش ۹۵ وزنش ۱۴ خوبه یا کمه   \n",
              "3  دخترم۳سالو نیمه قدش ۹۵ وزنش ۱۴ خوبه یا کمه   \n",
              "4  دخترم۳سالو نیمه قدش ۹۵ وزنش ۱۴ خوبه یا کمه   \n",
              "\n",
              "                                                   1  \n",
              "0  پسرم یه ماه دیگه میشه سه سالقدش حدودا دو ماه پ...  \n",
              "1       نمیدونم پسرم ماهشه قدش خودتون قدتون چجوریه ؟  \n",
              "2  من پسرم دوسال ونه ماهش هس قدوزن ترخدا بگید خوب...  \n",
              "3  عزیزم بچه هارو مقایسه نکن حتی خواهر بردار ها ه...  \n",
              "4  دختر منم سالشه قدش یه ماه پیش بود وزنش سیزده و...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2b51f90e-c965-46b4-954a-e30d51a0d398\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>دخترم۳سالو نیمه قدش ۹۵ وزنش ۱۴ خوبه یا کمه</td>\n",
              "      <td>پسرم یه ماه دیگه میشه سه سالقدش حدودا دو ماه پ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>دخترم۳سالو نیمه قدش ۹۵ وزنش ۱۴ خوبه یا کمه</td>\n",
              "      <td>نمیدونم پسرم ماهشه قدش خودتون قدتون چجوریه ؟</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>دخترم۳سالو نیمه قدش ۹۵ وزنش ۱۴ خوبه یا کمه</td>\n",
              "      <td>من پسرم دوسال ونه ماهش هس قدوزن ترخدا بگید خوب...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>دخترم۳سالو نیمه قدش ۹۵ وزنش ۱۴ خوبه یا کمه</td>\n",
              "      <td>عزیزم بچه هارو مقایسه نکن حتی خواهر بردار ها ه...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>دخترم۳سالو نیمه قدش ۹۵ وزنش ۱۴ خوبه یا کمه</td>\n",
              "      <td>دختر منم سالشه قدش یه ماه پیش بود وزنش سیزده و...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2b51f90e-c965-46b4-954a-e30d51a0d398')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2b51f90e-c965-46b4-954a-e30d51a0d398 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2b51f90e-c965-46b4-954a-e30d51a0d398');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1_p.to_csv('PerCQA_clean.csv')"
      ],
      "metadata": {
        "id": "z3exReJDjMOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.savetxt(r'PerCQA_formatted.txt', df1_p.values, fmt='%s', delimiter='\\t')"
      ],
      "metadata": {
        "id": "TqrAN3wbY5GC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# emoji_pattern = re.compile(\"[\"\n",
        "#         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "#         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "#         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "#         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "#                            \"]+\", flags=re.UNICODE)"
      ],
      "metadata": {
        "id": "GSM8B4Vwa3Gm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rx = re.compile(r'([!-/:-@[-`{-~])\\1+')\n",
        "# texto = rx.sub(r'\\1 ', 'sss..???. ... --- !!!!')\n",
        "# print(re. sub('\\s{2,}', ' ', texto))\n",
        "# print(len(texto))\n",
        "# print(texto)"
      ],
      "metadata": {
        "id": "66txD9gOhlf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 'g  gg '.strip()"
      ],
      "metadata": {
        "id": "eiTjG_fd6_2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# re.sub(r'\\s{2,}', r'', 'ممممکن نیست')"
      ],
      "metadata": {
        "id": "7ErUJrWj1jQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# re.sub(r'([\\u0600-\\u06FF\\s]){2,}', r'\\1', 'ممممکن نیست')"
      ],
      "metadata": {
        "id": "TJoTCwc40V2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def square(match):\n",
        "#     num = (match.group())\n",
        "#     print(match.group(1))\n",
        "#     return str(num)\n",
        "\n",
        "# l = ['ممممکن']\n",
        "# pattern = r'([\\u0600-\\u06FF\\s]){2,}'\n",
        "# new_l = [re.sub(pattern, square, s) for s in l]\n"
      ],
      "metadata": {
        "id": "Rg1r8Bvdz9Nj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# new_l"
      ],
      "metadata": {
        "id": "OwCalg9t3UDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# re.sub(r'([\\u0600-\\u06FF\\s])\\{2,}', r'\\1', 'ممممکن')"
      ],
      "metadata": {
        "id": "eGHVhVeDxLKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rx.sub(r'\\1 ', 'dada?//???? dd??')"
      ],
      "metadata": {
        "id": "0lo0PeTzwoRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df1 = pd.DataFrame()\n",
        "# df1['QBody'] = df_s['QBody'].apply(lambda row: re.sub('[^\\u0600-\\u06FF\\s]',\"\",normalizer.normalize(emoji_pattern.sub(r'', str(row).replace('\\n',' ')))))\n",
        "# df1['CBodyClean'] = df_s['CBodyClean'].apply(lambda row: re.sub('[^\\u0600-\\u06FF\\s]',\"\",normalizer.normalize(emoji_pattern.sub(r'', str(row).replace('\\n',' ')))))"
      ],
      "metadata": {
        "id": "Elq47cuza281"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rx = re.compile(r'([!-/:-@[-`{-~])\\1+')\n",
        "\n",
        "# df1['QBody'] = df1['QBody'].apply(lambda row: rx.sub(r'\\1 ', str(row)))\n",
        "# df1['CBodyClean'] = df1['CBodyClean'].apply(lambda row: rx.sub(r'\\1 ', str(row)))\n",
        "\n",
        "# df1['QBody'] = df1['QBody'].apply(lambda row: re.sub('\\s{2,}', ' ', str(row)).strip())\n",
        "# df1['CBodyClean'] = df1['CBodyClean'].apply(lambda row: re.sub('\\s{2,}', ' ', str(row)).strip())\n"
      ],
      "metadata": {
        "id": "ERz6L-9GjdHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df1.head(10)"
      ],
      "metadata": {
        "id": "9FXeLIxSgiao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ChitChat dataset"
      ],
      "metadata": {
        "id": "ayL6UUdF2zUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_ch = pd.read_csv('ChatbotData.csv',error_bad_lines = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWG9YsVY218t",
        "outputId": "eb816b3e-6aa8-448d-a63f-7bf32e4c3158"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-37-83937a3ab35b>:1: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  df_ch = pd.read_csv('ChatbotData.csv',error_bad_lines = False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_ch.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "4QGcyZY8210p",
        "outputId": "0cab1d87-e8f2-4bfe-b8ca-ed02405bb7c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                        Q             A\n",
              "0  در ھر صورت خیلی ممنون.  خواھش میکنم.\n",
              "1             سلام خوبین؟  سلام ممنونم.\n",
              "2             سلام کجایی؟    سلام خونم.\n",
              "3               اوکی مرسی         خواھش\n",
              "4               مرسی کاکو   فدات مخلصیم"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1c4897d0-e299-4071-8b4b-70dcc77f613a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Q</th>\n",
              "      <th>A</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>در ھر صورت خیلی ممنون.</td>\n",
              "      <td>خواھش میکنم.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>سلام خوبین؟</td>\n",
              "      <td>سلام ممنونم.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>سلام کجایی؟</td>\n",
              "      <td>سلام خونم.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>اوکی مرسی</td>\n",
              "      <td>خواھش</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>مرسی کاکو</td>\n",
              "      <td>فدات مخلصیم</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1c4897d0-e299-4071-8b4b-70dcc77f613a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1c4897d0-e299-4071-8b4b-70dcc77f613a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1c4897d0-e299-4071-8b4b-70dcc77f613a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.savetxt(r'Chitchat_pre_formatted.txt', df_ch.values, fmt='%s', delimiter='\\t')"
      ],
      "metadata": {
        "id": "vuuZ0WHI5hqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines_ch = open('Chitchat_pre_formatted.txt', encoding='utf-8').read().strip().split('\\n')\n",
        "pairs_ch = [[process(s) for s in l.split('\\t')] for l in lines_ch]"
      ],
      "metadata": {
        "id": "mct1DZqV5qua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs_ch[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGoeJN3V5wVQ",
        "outputId": "2cd34945-ef61-4a23-e5e3-884a49c186b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['در ھر صورت خیلی ممنون', 'خواھش میکنم']"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1_ch = pd.DataFrame(pairs_ch)"
      ],
      "metadata": {
        "id": "9_P8iyWl506x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1_ch.to_csv('chitchat_clean.csv')"
      ],
      "metadata": {
        "id": "1QemlJNFjpQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1_ch.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "QTiYs55xSB64",
        "outputId": "ad253d39-4b98-4afc-af99-0c8af992fc95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                       0            1\n",
              "0  در ھر صورت خیلی ممنون  خواھش میکنم\n",
              "1           سلام خوبین ؟  سلام ممنونم\n",
              "2           سلام کجایی ؟    سلام خونم\n",
              "3              اوکی مرسی        خواھش\n",
              "4              مرسی کاکو  فدات مخلصیم"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1f055d56-4cdf-4a50-b082-94b99bd3b085\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>در ھر صورت خیلی ممنون</td>\n",
              "      <td>خواھش میکنم</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>سلام خوبین ؟</td>\n",
              "      <td>سلام ممنونم</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>سلام کجایی ؟</td>\n",
              "      <td>سلام خونم</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>اوکی مرسی</td>\n",
              "      <td>خواھش</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>مرسی کاکو</td>\n",
              "      <td>فدات مخلصیم</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1f055d56-4cdf-4a50-b082-94b99bd3b085')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1f055d56-4cdf-4a50-b082-94b99bd3b085 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1f055d56-4cdf-4a50-b082-94b99bd3b085');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.savetxt(r'Chitchat_formatted.txt', df1_ch.values, fmt='%s', delimiter='\\t')"
      ],
      "metadata": {
        "id": "fa4DHIYp6Ci9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Combination of PerCQA and chitchat"
      ],
      "metadata": {
        "id": "jsYl6hBCOI3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1_comb = pd.concat([df1_ch,df1_p])"
      ],
      "metadata": {
        "id": "Igw4-tzDOPvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1_comb.iloc[2023][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hka5-KJtOPsP",
        "outputId": "3187f854-7a0e-421d-937b-1ff7cfec9a8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'به بچه های دو سالتون چی یاد میدین ؟ من نمیدونم دیگه باید چی یادش بدم ؟'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.savetxt(r'Combination_formatted.txt', df1_comb.values, fmt='%s', delimiter='\\t')"
      ],
      "metadata": {
        "id": "_Tdx72mBOPnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1_comb.to_csv('chitchat_percqa_clean.csv')"
      ],
      "metadata": {
        "id": "1IwwMM4GjzCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset Configs"
      ],
      "metadata": {
        "id": "_GQj1t2yA8cp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    'PerCQA' : {\n",
        "        'corpus_name' : \"ninsite\", \n",
        "        'datafile' : \"PerCQA_formatted.txt\",\n",
        "        'MAX_LENGTH' : 80,\n",
        "        'MIN_COUNT' : 3,\n",
        "        'model_name' : 'p_model',\n",
        "        'attn_model' : 'dot',\n",
        "        'hidden_size' : 500,\n",
        "        'encoder_n_layers' : 2,\n",
        "        'decoder_n_layers' : 2,\n",
        "        'dropout' : 0.1,\n",
        "        'batch_size' : 64,\n",
        "        'train' : {\n",
        "            'clip' : 50.0,\n",
        "            'teacher_forcing_ratio' : 1.0,\n",
        "            'learning_rate' : 0.0001,\n",
        "            'decoder_learning_ratio' : 5.0,\n",
        "            'n_iteration' : 3000,\n",
        "            'print_every' : 100,\n",
        "            'save_every' : 100\n",
        "\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "    },\n",
        "    'Chitchat' : {\n",
        "        'corpus_name' : \"chitchat\", \n",
        "        'datafile' : \"Chitchat_formatted.txt\",\n",
        "        'MAX_LENGTH' : 10,\n",
        "        'MIN_COUNT' : 2,\n",
        "        'model_name' : 'ch_model',\n",
        "        'attn_model' : 'dot',\n",
        "        'hidden_size' : 500,\n",
        "        'encoder_n_layers' : 2,\n",
        "        'decoder_n_layers' : 2,\n",
        "        'dropout' : 0.1,\n",
        "        'batch_size' : 64,\n",
        "        'train' : {\n",
        "            'clip' : 50.0,\n",
        "            'teacher_forcing_ratio' : 1.0,\n",
        "            'learning_rate' : 0.0001,\n",
        "            'decoder_learning_ratio' : 5.0,\n",
        "            'n_iteration' : 400,\n",
        "            'print_every' : 20,\n",
        "            'save_every' : 50\n",
        "\n",
        "        }\n",
        "\n",
        "    },\n",
        "    'PerCQA_Chitchat' : {\n",
        "        'corpus_name' : \"comb\", \n",
        "        'datafile' : \"Combination_formatted.txt\",\n",
        "        'MAX_LENGTH' : 30,\n",
        "        'MIN_COUNT' : 2,\n",
        "        'model_name' : 'cb_model',\n",
        "        'attn_model' : 'dot',\n",
        "        'hidden_size' : 500,\n",
        "        'encoder_n_layers' : 2,\n",
        "        'decoder_n_layers' : 2,\n",
        "        'dropout' : 0.1,\n",
        "        'batch_size' : 64,\n",
        "        'train' : {\n",
        "            'clip' : 50.0,\n",
        "            'teacher_forcing_ratio' : 1.0,\n",
        "            'learning_rate' : 0.0001,\n",
        "            'decoder_learning_ratio' : 5.0,\n",
        "            'n_iteration' : 4000,\n",
        "            'print_every' : 10,\n",
        "            'save_every' : 50\n",
        "\n",
        "        }\n",
        "\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "4F_Qq-fFBAjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82Syxa5fDv1N"
      },
      "source": [
        "Load & Preprocess Data\n",
        "----------------------\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset that use for training\n",
        "DATASET = 'PerCQA_Chitchat'\n",
        "CONFIG = config[DATASET]\n",
        "TRAIN_CONFIG = CONFIG['train']"
      ],
      "metadata": {
        "id": "dUkOGVF9UTvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6hA3hgwDv1N"
      },
      "outputs": [],
      "source": [
        "def printLines(file, n=10):\n",
        "    with open(file, 'rb') as datafile:\n",
        "        lines = datafile.readlines()\n",
        "    for line in lines[:n]:\n",
        "        print(line)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4gPAHSGDv1O"
      },
      "source": [
        "# Load and trim data\n",
        "~~~~~~~~~~~~~~~~~~\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ye2Oe5sKDv1P"
      },
      "outputs": [],
      "source": [
        "# Default word tokens\n",
        "PAD_token = 0  # Used for padding short sentences\n",
        "SOS_token = 1  # Start-of-sentence token\n",
        "EOS_token = 2  # End-of-sentence token\n",
        "UNK_token = 3  # Unknown token\n",
        "\n",
        "class Voc:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.trimmed = False\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\", \n",
        "                           UNK_token: \"UNK\" }\n",
        "        self.num_words = 4  # Count SOS, EOS, PAD, UNK\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.num_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.num_words] = word\n",
        "            self.num_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "    # Remove words below a certain count threshold\n",
        "    def trim(self, min_count):\n",
        "        if self.trimmed:\n",
        "            return\n",
        "        self.trimmed = True\n",
        "\n",
        "        keep_words = []\n",
        "\n",
        "        for k, v in self.word2count.items():\n",
        "            if v >= min_count:\n",
        "                keep_words.append(k)\n",
        "\n",
        "        print('keep_words {} / {} = {:.4f}'.format(\n",
        "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
        "        ))\n",
        "\n",
        "        # Reinitialize dictionaries\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\", \n",
        "                           UNK_token: \"UNK\" }\n",
        "        self.num_words = 4  # Count SOS, EOS, PAD, UNK\n",
        "\n",
        "        for word in keep_words:\n",
        "            self.addWord(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdkZir5PDv1R",
        "outputId": "e254259b-bbfd-420e-bbcd-8bf09a15e88e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start preparing training data ...\n",
            "Reading lines...\n",
            "Read 22850 sentence pairs\n",
            "Trimmed to 9579 sentence pairs\n",
            "Counting words...\n",
            "Counted words: 14041\n",
            "\n",
            "pairs:\n",
            "['در ھر صورت خیلی ممنون', 'خواھش میکنم']\n",
            "['سلام خوبین ؟', 'سلام ممنونم']\n",
            "['سلام کجایی ؟', 'سلام خونم']\n",
            "['اوکی مرسی', 'خواھش']\n",
            "['مرسی کاکو', 'فدات مخلصیم']\n",
            "['سلام خوبین ؟', 'سلام ممنون شما خوبین ؟']\n",
            "['ی سوال دارم', 'بفرمایید']\n",
            "['خیلی ممنونم ، برم بررسیشون کنم', 'موفق باشید']\n",
            "['سلام میشه بپرسم تمرینتون چیه ؟', 'سلام بله اجازه بدید']\n",
            "['ببخشید نمیرسم انجامش بدم', 'خواهش میکنم مشکلی نیست']\n"
          ]
        }
      ],
      "source": [
        "MAX_LENGTH = CONFIG['MAX_LENGTH'] # Maximum sentence length to consider\n",
        "\n",
        "\n",
        "# Read query/response pairs and return a voc object\n",
        "def readVocs(datafile, corpus_name):\n",
        "    print(\"Reading lines...\")\n",
        "    # Read the file and split into lines\n",
        "    lines = open(datafile, encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[s for s in l.split('\\t')] for l in lines]\n",
        "    voc = Voc(corpus_name)\n",
        "    return voc, pairs\n",
        "\n",
        "# Returns True iff both sentences in a pair 'p' are under the MAX_LENGTH threshold\n",
        "def filterPair(p):\n",
        "    # Input sequences need to preserve the last word for EOS token\n",
        "    if len(p) <= 1:\n",
        "        return False\n",
        "    if len(p[0]) == 0 or len(p[1]) == 0:\n",
        "        return False\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n",
        "\n",
        "# Filter pairs using filterPair condition\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]\n",
        "\n",
        "# Using the functions defined above, return a populated voc object and pairs list\n",
        "def loadPrepareData(corpus_name, datafile, save_dir):\n",
        "    print(\"Start preparing training data ...\")\n",
        "    voc, pairs = readVocs(datafile, corpus_name)\n",
        "    print(\"Read {!s} sentence pairs\".format(len(pairs)))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to {!s} sentence pairs\".format(len(pairs)))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        voc.addSentence(pair[0])\n",
        "        voc.addSentence(pair[1])\n",
        "    print(\"Counted words:\", voc.num_words)\n",
        "    return voc, pairs\n",
        "\n",
        "\n",
        "# Load/Assemble voc and pairs\n",
        "save_dir = os.path.join(\"data\", \"save\")\n",
        "corpus_name = CONFIG['corpus_name']\n",
        "datafile = CONFIG['datafile']\n",
        "\n",
        "voc, pairs = loadPrepareData ( corpus_name, datafile, save_dir)\n",
        "# Print some pairs to validate\n",
        "print(\"\\npairs:\")\n",
        "for pair in pairs[:10]:\n",
        "    print(pair)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuIucSIoDv1S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33f3cbe6-a4da-49ab-d719-70d2a8488a33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keep_words 6466 / 14037 = 0.4606\n",
            "Trimmed from 9579 pairs to 5224, 0.5454 of total\n"
          ]
        }
      ],
      "source": [
        "MIN_COUNT = CONFIG['MIN_COUNT']   # Minimum word count threshold for trimming\n",
        "\n",
        "def trimRareWords(voc, pairs, MIN_COUNT):\n",
        "    # Trim words used under the MIN_COUNT from the voc\n",
        "    voc.trim(MIN_COUNT)\n",
        "    # Filter out pairs with trimmed words\n",
        "    keep_pairs = []\n",
        "    for pair in pairs:\n",
        "        input_sentence = pair[0]\n",
        "        output_sentence = pair[1]\n",
        "        keep_input = True\n",
        "        keep_output = True\n",
        "        # Check input sentence\n",
        "        for word in input_sentence.split(' '):\n",
        "            if word not in voc.word2index:\n",
        "                keep_input = False\n",
        "                break\n",
        "        # Check output sentence\n",
        "        for word in output_sentence.split(' '):\n",
        "            if word not in voc.word2index:\n",
        "                keep_output = False\n",
        "                break\n",
        "\n",
        "        # Only keep pairs that do not contain trimmed word(s) in their input or output sentence\n",
        "        if keep_input and keep_output:\n",
        "            keep_pairs.append(pair)\n",
        "\n",
        "    print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n",
        "    return keep_pairs\n",
        "\n",
        "\n",
        "# Trim voc and pairs\n",
        "pairs = trimRareWords(voc, pairs, MIN_COUNT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ep3SQMsDv1T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75404ab9-2556-456b-a1ab-5074795cbbb8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5224"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "len(pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mK52VVagDv1U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08a00f13-c1b5-4a55-9f00-e568ac5d13aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "voc.word2index['سلام']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODKIbHZyDv1V",
        "outputId": "e5cb5c96-0e54-4ca3-848d-d746558fb052"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_variable: tensor([[1459,   77, 3449,  384,  384],\n",
            "        [ 243, 1406, 3023, 2627,  208],\n",
            "        [  84,  136,  499, 2995,  147],\n",
            "        [2053, 1692, 5977, 2996, 5328],\n",
            "        [1520,  731,  602,  243,  330],\n",
            "        [1649,   13, 1415,  338, 1321],\n",
            "        [  77, 1185, 4800,  313, 5329],\n",
            "        [  78, 1692,  624, 5128,  269],\n",
            "        [ 269,  821,   13,    8, 3260],\n",
            "        [ 103,  179, 5978,  444,  210],\n",
            "        [2054,   60,  133, 1324,   13],\n",
            "        [2055,   13,  147, 2334,    2],\n",
            "        [  56, 6287, 5979,  465,    0],\n",
            "        [1520,   13, 3441,    2,    0],\n",
            "        [ 989,  512,   13,    0,    0],\n",
            "        [ 998,   30,    2,    0,    0],\n",
            "        [  50,  488,    0,    0,    0],\n",
            "        [1785, 1644,    0,    0,    0],\n",
            "        [ 621,   13,    0,    0,    0],\n",
            "        [2056,    2,    0,    0,    0],\n",
            "        [ 208,    0,    0,    0,    0],\n",
            "        [2057,    0,    0,    0,    0],\n",
            "        [2058,    0,    0,    0,    0],\n",
            "        [ 242,    0,    0,    0,    0],\n",
            "        [ 512,    0,    0,    0,    0],\n",
            "        [  30,    0,    0,    0,    0],\n",
            "        [  13,    0,    0,    0,    0],\n",
            "        [   2,    0,    0,    0,    0]])\n",
            "lengths: tensor([28, 20, 16, 14, 12])\n",
            "target_variable: tensor([[ 237,   77,   94,    7,  393],\n",
            "        [1520,   56,  908,   14,   25],\n",
            "        [ 269,  570, 1440,    2, 1432],\n",
            "        [ 243,  721, 1715,    0,  384],\n",
            "        [ 313,   55, 3023,    0, 1101],\n",
            "        [  61,   55, 1383,    0, 3260],\n",
            "        [1314, 4779, 1003,    0,  210],\n",
            "        [  33, 4779,  140,    0,  143],\n",
            "        [1878, 1692, 4803,    0,    2],\n",
            "        [1266, 1038,    2,    0,    0],\n",
            "        [2104,  208,    0,    0,    0],\n",
            "        [  76,  103,    0,    0,    0],\n",
            "        [1665,  488,    0,    0,    0],\n",
            "        [1674, 1294,    0,    0,    0],\n",
            "        [  13,    2,    0,    0,    0],\n",
            "        [   2,    0,    0,    0,    0]])\n",
            "mask: tensor([[ True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True, False,  True],\n",
            "        [ True,  True,  True, False,  True],\n",
            "        [ True,  True,  True, False,  True],\n",
            "        [ True,  True,  True, False,  True],\n",
            "        [ True,  True,  True, False,  True],\n",
            "        [ True,  True,  True, False,  True],\n",
            "        [ True,  True,  True, False, False],\n",
            "        [ True,  True, False, False, False],\n",
            "        [ True,  True, False, False, False],\n",
            "        [ True,  True, False, False, False],\n",
            "        [ True,  True, False, False, False],\n",
            "        [ True,  True, False, False, False],\n",
            "        [ True, False, False, False, False]])\n",
            "max_target_len: 16\n"
          ]
        }
      ],
      "source": [
        "def indexesFromSentence(voc, sentence):\n",
        "    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n",
        "\n",
        "\n",
        "def zeroPadding(l, fillvalue=PAD_token):\n",
        "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
        "\n",
        "def binaryMatrix(l, value=PAD_token):\n",
        "    m = []\n",
        "    for i, seq in enumerate(l):\n",
        "        m.append([])\n",
        "        for token in seq:\n",
        "            if token == PAD_token:\n",
        "                m[i].append(0)\n",
        "            else:\n",
        "                m[i].append(1)\n",
        "    return m\n",
        "\n",
        "# Returns padded input sequence tensor and lengths\n",
        "def inputVar(l, voc):\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "    padList = zeroPadding(indexes_batch)\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar, lengths\n",
        "\n",
        "# Returns padded target sequence tensor, padding mask, and max target length\n",
        "def outputVar(l, voc):\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
        "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
        "    padList = zeroPadding(indexes_batch)\n",
        "    mask = binaryMatrix(padList)\n",
        "    mask = torch.BoolTensor(mask)\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar, mask, max_target_len\n",
        "\n",
        "# Returns all items for a given batch of pairs\n",
        "def batch2TrainData(voc, pair_batch):\n",
        "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
        "    input_batch, output_batch = [], []\n",
        "    for pair in pair_batch:\n",
        "        input_batch.append(pair[0])\n",
        "        output_batch.append(pair[1])\n",
        "    inp, lengths = inputVar(input_batch, voc)\n",
        "    output, mask, max_target_len = outputVar(output_batch, voc)\n",
        "    return inp, lengths, output, mask, max_target_len\n",
        "\n",
        "\n",
        "# Example for validation\n",
        "small_batch_size = 5\n",
        "batches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])\n",
        "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
        "\n",
        "print(\"input_variable:\", input_variable)\n",
        "print(\"lengths:\", lengths)\n",
        "print(\"target_variable:\", target_variable)\n",
        "print(\"mask:\", mask)\n",
        "print(\"max_target_len:\", max_target_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69mRSCK9Dv1W",
        "outputId": "1fe17640-274f-4a2c-a896-20a712d30faf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['دادن سیپرو برای اشتهای بچه ضرر داره ؟ اصلا اشتها نداره ماهشه کلی شربت اشتها هم دادم ولی فایده نداشته',\n",
              "  'چقدر میدی چجوری ؟ بچه چند وقتشه ؟'],\n",
              " ['شیرخشک پپتی جونیوره نمیخوره گفتم شاید مزش عوض بشه بخوره',\n",
              "  'مگه خودت میخای بخوری ؟'],\n",
              " ['شما چه قطره آهنی برای کوچولوهاتون استفاده می کنین ؟؟ راضی هستین می خوره بالا نمیاره و دندونشو سیاه نمی کنه ؟ قیمتش رو هم بگین ممنون',\n",
              "  'تو داروخانه بری بگی راهنماییم کنین بهتر از اینجا راهنمایی میکنن چون بلد هستن'],\n",
              " ['بهترین جنس قابلمه چیه برای جهیزیه میخوام اصلا قیمتش مهم نیست میخوام از هر لحاظ عالی ترین باشه',\n",
              "  'گرانیت'],\n",
              " ['بچهه ها میدونین مهمون امشب دور همى کیه ؟؟',\n",
              "  'محمود کلاری حسش نبود خاموشیدم']]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "[random.choice(pairs) for _ in range(small_batch_size)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDDcQeRfDv1X"
      },
      "source": [
        "# Define Models\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CaBr-dPDv1X"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upTAc_7aDv1Y"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0, block_type = 'gru'):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = embedding\n",
        "        self.block_type = block_type\n",
        "        # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size'\n",
        "        #   because our input size is a word embedding with number of features == hidden_size\n",
        "        if block_type == 'gru' :\n",
        "            self.block = nn.GRU(hidden_size, hidden_size, n_layers,\n",
        "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
        "        else:\n",
        "            self.block = nn.LSTM(hidden_size, hidden_size, n_layers,\n",
        "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
        "\n",
        "        \n",
        "    def forward(self, input_seq, input_lengths, hidden=None):\n",
        "        # Convert word indexes to embeddings\n",
        "        embedded = self.embedding(input_seq)\n",
        "        # Pack padded batch of sequences for RNN module\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
        "        # Forward pass through Block\n",
        "        if self.block_type == 'gru':\n",
        "            outputs, hidden = self.block(packed, hidden)\n",
        "        else:\n",
        "            #inthis case hodden is a tuple that contains both last hidden state and last cell state\n",
        "            outputs, hidden = self.block(packed, hidden)\n",
        "        # Unpack padding\n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
        "        # Sum bidirectional Block outputs\n",
        "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
        "        # Return output and final hidden state\n",
        "        return outputs, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQNQ4SrwDv1Y"
      },
      "source": [
        "## Decoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_RR0N3VDv1Z"
      },
      "outputs": [],
      "source": [
        "# Luong attention layer\n",
        "class Attn(nn.Module):\n",
        "    def __init__(self, method, hidden_size):\n",
        "        super(Attn, self).__init__()\n",
        "        self.method = method\n",
        "        if self.method not in ['dot', 'general', 'concat']:\n",
        "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
        "        self.hidden_size = hidden_size\n",
        "        if self.method == 'general':\n",
        "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
        "        elif self.method == 'concat':\n",
        "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
        "            self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
        "\n",
        "    def dot_score(self, hidden, encoder_output):\n",
        "        return torch.sum(hidden * encoder_output, dim=2)\n",
        "\n",
        "    def general_score(self, hidden, encoder_output):\n",
        "        energy = self.attn(encoder_output)\n",
        "        return torch.sum(hidden * energy, dim=2)\n",
        "\n",
        "    def concat_score(self, hidden, encoder_output):\n",
        "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
        "        return torch.sum(self.v * energy, dim=2)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # Calculate the attention weights (energies) based on the given method\n",
        "        if self.method == 'general':\n",
        "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
        "        elif self.method == 'concat':\n",
        "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
        "        elif self.method == 'dot':\n",
        "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
        "\n",
        "        # Transpose max_length and batch_size dimensions\n",
        "        attn_energies = attn_energies.t()\n",
        "\n",
        "        # Return the softmax normalized probability scores (with added dimension)\n",
        "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54H1eULsDv1c"
      },
      "outputs": [],
      "source": [
        "class LuongAttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1, block_type='gru'):\n",
        "        super(LuongAttnDecoderRNN, self).__init__()\n",
        "\n",
        "        # Keep for reference\n",
        "        self.attn_model = attn_model\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "        self.block_type = block_type\n",
        "        \n",
        "        # Define layers\n",
        "        self.embedding = embedding\n",
        "        self.embedding_dropout = nn.Dropout(dropout)\n",
        "        if block_type == 'gru' :\n",
        "            self.block = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
        "        \n",
        "        else:\n",
        "            self.block = nn.LSTM(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
        "        \n",
        "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        self.attn = Attn(attn_model, hidden_size)\n",
        "\n",
        "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
        "        # Note: we run this one step (word) at a time\n",
        "        # Get embedding of current input word\n",
        "        embedded = self.embedding(input_step)\n",
        "        embedded = self.embedding_dropout(embedded)\n",
        "        # print('there')\n",
        "        # print(embedded.shape)\n",
        "        # print(last_hidden[0].shape)\n",
        "\n",
        "        # Forward through unidirectional block\n",
        "        if self.block_type == 'gru':\n",
        "            rnn_output, hidden = self.block(embedded, last_hidden)\n",
        "        \n",
        "        else:\n",
        "            rnn_output, (hidden) = self.block(embedded, last_hidden)\n",
        "        \n",
        "        \n",
        "        # Calculate attention weights from the current GRU output\n",
        "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
        "        # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n",
        "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
        "        # Concatenate weighted context vector and GRU output using Luong eq. 5\n",
        "        rnn_output = rnn_output.squeeze(0)\n",
        "        context = context.squeeze(1)\n",
        "        concat_input = torch.cat((rnn_output, context), 1)\n",
        "        concat_output = torch.tanh(self.concat(concat_input))\n",
        "        # Predict next word using Luong eq. 6\n",
        "        output = self.out(concat_output)\n",
        "        output = F.softmax(output, dim=1)\n",
        "        # Return output and final hidden state\n",
        "        return output, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHdFhiX-Dv1c"
      },
      "source": [
        "# Define Training Procedure\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Am2HNVLQMqQx"
      },
      "source": [
        "## Masked loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mH6cMw5dDv1d"
      },
      "outputs": [],
      "source": [
        "def maskNLLLoss(inp, target, mask):\n",
        "    nTotal = mask.sum()\n",
        "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
        "    loss = crossEntropy.masked_select(mask).mean()\n",
        "    loss = loss.to(device)\n",
        "    return loss, nTotal.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRJk3akUDv1e"
      },
      "outputs": [],
      "source": [
        "def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding,\n",
        "          encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=MAX_LENGTH):\n",
        "\n",
        "    # Zero gradients\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    # Set device options\n",
        "    input_variable = input_variable.to(device)\n",
        "    target_variable = target_variable.to(device)\n",
        "    mask = mask.to(device)\n",
        "    # Lengths for rnn packing should always be on the cpu\n",
        "    lengths = lengths.to(\"cpu\")\n",
        "\n",
        "    # Initialize variables\n",
        "    loss = 0\n",
        "    print_losses = []\n",
        "    n_totals = 0\n",
        "\n",
        "    # Forward pass through encoder\n",
        "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
        "\n",
        "    # Create initial decoder input (start with SOS tokens for each sentence)\n",
        "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
        "    decoder_input = decoder_input.to(device)\n",
        "\n",
        "    # Set initial decoder hidden state to the encoder's final hidden state\n",
        "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "\n",
        "    # Determine if we are using teacher forcing this iteration\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    # Forward batch of sequences through decoder one time step at a time\n",
        "    if use_teacher_forcing:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            # Teacher forcing: next input is current target\n",
        "            decoder_input = target_variable[t].view(1, -1)\n",
        "            # Calculate and accumulate loss\n",
        "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "    else:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            # No teacher forcing: next input is decoder's own current output\n",
        "            _, topi = decoder_output.topk(1)\n",
        "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
        "            decoder_input = decoder_input.to(device)\n",
        "            # Calculate and accumulate loss\n",
        "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "\n",
        "    # Perform backpropatation\n",
        "    loss.backward()\n",
        "\n",
        "    # Clip gradients: gradients are modified in place\n",
        "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
        "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
        "\n",
        "    # Adjust model weights\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return sum(print_losses) / n_totals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-K6zHIiuDv1f"
      },
      "outputs": [],
      "source": [
        "def trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename):\n",
        "\n",
        "    # Load batches for each iteration\n",
        "    training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
        "                      for _ in range(n_iteration)]\n",
        "\n",
        "    # Initializations\n",
        "    print('Initializing ...')\n",
        "    start_iteration = 1\n",
        "    print_loss = 0\n",
        "    if loadFilename:\n",
        "        start_iteration = checkpoint['iteration'] + 1\n",
        "\n",
        "    # Training loop\n",
        "    print(\"Training...\")\n",
        "    for iteration in range(start_iteration, n_iteration + 1):\n",
        "        training_batch = training_batches[iteration - 1]\n",
        "        # Extract fields from batch\n",
        "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
        "\n",
        "        # Run a training iteration with batch\n",
        "        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n",
        "                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
        "        print_loss += loss\n",
        "\n",
        "        # Print progress\n",
        "        if iteration % print_every == 0:\n",
        "            print_loss_avg = print_loss / print_every\n",
        "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
        "            print_loss = 0\n",
        "\n",
        "        # Save checkpoint\n",
        "        if (iteration % save_every == 0):\n",
        "            directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
        "            if not os.path.exists(directory):\n",
        "                os.makedirs(directory)\n",
        "            torch.save({\n",
        "                'iteration': iteration,\n",
        "                'en': encoder.state_dict(),\n",
        "                'de': decoder.state_dict(),\n",
        "                'en_opt': encoder_optimizer.state_dict(),\n",
        "                'de_opt': decoder_optimizer.state_dict(),\n",
        "                'loss': loss,\n",
        "                'voc_dict': voc.__dict__,\n",
        "                'embedding': embedding.state_dict()\n",
        "            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zF7FdnqDv1f"
      },
      "source": [
        "Define Evaluation\n",
        "-----------------\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyOq1XEpDv1g"
      },
      "outputs": [],
      "source": [
        "class GreedySearchDecoder(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(GreedySearchDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, input_seq, input_length, max_length):\n",
        "        # Forward input through encoder model\n",
        "        print(type(input_seq))\n",
        "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
        "        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
        "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "        # Initialize decoder input with SOS_token\n",
        "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
        "        # Initialize tensors to append decoded words to\n",
        "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
        "        all_scores = torch.zeros([0], device=device)\n",
        "        # Iteratively decode one word token at a time\n",
        "        for _ in range(max_length):\n",
        "            # Forward pass through decoder\n",
        "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            # Obtain most likely word token and its softmax score\n",
        "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
        "            # Record token and score\n",
        "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
        "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
        "            # Prepare current token to be next decoder input (add a dimension)\n",
        "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
        "        # Return collections of word tokens and scores\n",
        "        return all_tokens, all_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYFMKdipDv1g"
      },
      "outputs": [],
      "source": [
        "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n",
        "    ### Format input sentence as a batch\n",
        "    # words -> indexes\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence)]\n",
        "    # Create lengths tensor\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "    # Transpose dimensions of batch to match models' expectations\n",
        "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
        "    # Use appropriate device\n",
        "    input_batch = input_batch.to(device)\n",
        "    # lengths = lengths.to(device)\n",
        "    # Decode sentence with searcher\n",
        "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
        "    # indexes -> words\n",
        "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
        "    return decoded_words\n",
        "\n",
        "\n",
        "def evaluateInput(encoder, decoder, searcher, voc):\n",
        "    input_sentence = ''\n",
        "    while(1):\n",
        "        try:\n",
        "            # Get input sentence\n",
        "            input_sentence = input('> ')\n",
        "            # Check if it is quit case\n",
        "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
        "            # Normalize sentence\n",
        "            input_sentence = process(input_sentence)\n",
        "            # Evaluate sentence\n",
        "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
        "            # Format and print response sentence\n",
        "            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
        "            print('Bot:', ' '.join(output_words))\n",
        "\n",
        "        except KeyError:\n",
        "            print(\"Error: Encountered unknown word.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGI_JxcADv1h"
      },
      "source": [
        "Run Model\n",
        "---------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKav7PKvDv1h",
        "outputId": "bc9216cb-4d1e-48af-ab55-6ee9aa2a407c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building encoder and decoder ...\n",
            "Models built and ready to go!\n"
          ]
        }
      ],
      "source": [
        "# Configure models\n",
        "model_name = CONFIG['model_name']\n",
        "attn_model = CONFIG['attn_model']\n",
        "#attn_model = 'general'\n",
        "#attn_model = 'concat'\n",
        "hidden_size = CONFIG['hidden_size']\n",
        "encoder_n_layers = CONFIG['encoder_n_layers']\n",
        "decoder_n_layers = CONFIG['decoder_n_layers']\n",
        "dropout = CONFIG['dropout']\n",
        "batch_size = CONFIG['batch_size']\n",
        "\n",
        "# Set checkpoint to load from; set to None if starting from scratch\n",
        "loadFilename = None\n",
        "checkpoint_iter = 4000\n",
        "#loadFilename = os.path.join(save_dir, model_name, corpus_name,\n",
        "#                            '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size),\n",
        "#                            '{}_checkpoint.tar'.format(checkpoint_iter))\n",
        "\n",
        "\n",
        "# Load model if a loadFilename is provided\n",
        "if loadFilename:\n",
        "    # If loading on same machine the model was trained on\n",
        "    checkpoint = torch.load(loadFilename)\n",
        "    # If loading a model trained on GPU to CPU\n",
        "    #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
        "    encoder_sd = checkpoint['en']\n",
        "    decoder_sd = checkpoint['de']\n",
        "    encoder_optimizer_sd = checkpoint['en_opt']\n",
        "    decoder_optimizer_sd = checkpoint['de_opt']\n",
        "    embedding_sd = checkpoint['embedding']\n",
        "    voc.__dict__ = checkpoint['voc_dict']\n",
        "\n",
        "\n",
        "print('Building encoder and decoder ...')\n",
        "# Initialize word embeddings\n",
        "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
        "if loadFilename:\n",
        "    embedding.load_state_dict(embedding_sd)\n",
        "# Initialize encoder & decoder models\n",
        "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
        "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
        "if loadFilename:\n",
        "    encoder.load_state_dict(encoder_sd)\n",
        "    decoder.load_state_dict(decoder_sd)\n",
        "# Use appropriate device\n",
        "encoder = encoder.to(device)\n",
        "decoder = decoder.to(device)\n",
        "print('Models built and ready to go!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwHDPs7eDv1i",
        "outputId": "8d909a1e-a7ca-4fc5-d7af-04c381cf4b69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building optimizers ...\n",
            "Starting Training!\n",
            "Initializing ...\n",
            "Training...\n",
            "Iteration: 10; Percent complete: 0.2%; Average loss: 8.4109\n",
            "Iteration: 20; Percent complete: 0.5%; Average loss: 7.5192\n",
            "Iteration: 30; Percent complete: 0.8%; Average loss: 6.8372\n",
            "Iteration: 40; Percent complete: 1.0%; Average loss: 6.8903\n",
            "Iteration: 50; Percent complete: 1.2%; Average loss: 6.8070\n",
            "Iteration: 60; Percent complete: 1.5%; Average loss: 6.7663\n",
            "Iteration: 70; Percent complete: 1.8%; Average loss: 6.7061\n",
            "Iteration: 80; Percent complete: 2.0%; Average loss: 6.6840\n",
            "Iteration: 90; Percent complete: 2.2%; Average loss: 6.6432\n",
            "Iteration: 100; Percent complete: 2.5%; Average loss: 6.5869\n",
            "Iteration: 110; Percent complete: 2.8%; Average loss: 6.5344\n",
            "Iteration: 120; Percent complete: 3.0%; Average loss: 6.5193\n",
            "Iteration: 130; Percent complete: 3.2%; Average loss: 6.5153\n",
            "Iteration: 140; Percent complete: 3.5%; Average loss: 6.4941\n",
            "Iteration: 150; Percent complete: 3.8%; Average loss: 6.4760\n",
            "Iteration: 160; Percent complete: 4.0%; Average loss: 6.4283\n",
            "Iteration: 170; Percent complete: 4.2%; Average loss: 6.3319\n",
            "Iteration: 180; Percent complete: 4.5%; Average loss: 6.3626\n",
            "Iteration: 190; Percent complete: 4.8%; Average loss: 6.3635\n",
            "Iteration: 200; Percent complete: 5.0%; Average loss: 6.3260\n",
            "Iteration: 210; Percent complete: 5.2%; Average loss: 6.3155\n",
            "Iteration: 220; Percent complete: 5.5%; Average loss: 6.2899\n",
            "Iteration: 230; Percent complete: 5.8%; Average loss: 6.2535\n",
            "Iteration: 240; Percent complete: 6.0%; Average loss: 6.2021\n",
            "Iteration: 250; Percent complete: 6.2%; Average loss: 6.1764\n",
            "Iteration: 260; Percent complete: 6.5%; Average loss: 6.1353\n",
            "Iteration: 270; Percent complete: 6.8%; Average loss: 6.1373\n",
            "Iteration: 280; Percent complete: 7.0%; Average loss: 6.0976\n",
            "Iteration: 290; Percent complete: 7.2%; Average loss: 6.1221\n",
            "Iteration: 300; Percent complete: 7.5%; Average loss: 6.0108\n",
            "Iteration: 310; Percent complete: 7.8%; Average loss: 6.0082\n",
            "Iteration: 320; Percent complete: 8.0%; Average loss: 5.8393\n",
            "Iteration: 330; Percent complete: 8.2%; Average loss: 5.9703\n",
            "Iteration: 340; Percent complete: 8.5%; Average loss: 5.9178\n",
            "Iteration: 350; Percent complete: 8.8%; Average loss: 5.7593\n",
            "Iteration: 360; Percent complete: 9.0%; Average loss: 5.7604\n",
            "Iteration: 370; Percent complete: 9.2%; Average loss: 5.7655\n",
            "Iteration: 380; Percent complete: 9.5%; Average loss: 5.7566\n",
            "Iteration: 390; Percent complete: 9.8%; Average loss: 5.6831\n",
            "Iteration: 400; Percent complete: 10.0%; Average loss: 5.7104\n",
            "Iteration: 410; Percent complete: 10.2%; Average loss: 5.6651\n",
            "Iteration: 420; Percent complete: 10.5%; Average loss: 5.6651\n",
            "Iteration: 430; Percent complete: 10.8%; Average loss: 5.5626\n",
            "Iteration: 440; Percent complete: 11.0%; Average loss: 5.5624\n",
            "Iteration: 450; Percent complete: 11.2%; Average loss: 5.5426\n",
            "Iteration: 460; Percent complete: 11.5%; Average loss: 5.4533\n",
            "Iteration: 470; Percent complete: 11.8%; Average loss: 5.4683\n",
            "Iteration: 480; Percent complete: 12.0%; Average loss: 5.3866\n",
            "Iteration: 490; Percent complete: 12.2%; Average loss: 5.3798\n",
            "Iteration: 500; Percent complete: 12.5%; Average loss: 5.3014\n",
            "Iteration: 510; Percent complete: 12.8%; Average loss: 5.3231\n",
            "Iteration: 520; Percent complete: 13.0%; Average loss: 5.2876\n",
            "Iteration: 530; Percent complete: 13.2%; Average loss: 5.2767\n",
            "Iteration: 540; Percent complete: 13.5%; Average loss: 5.1819\n",
            "Iteration: 550; Percent complete: 13.8%; Average loss: 5.2118\n",
            "Iteration: 560; Percent complete: 14.0%; Average loss: 5.1371\n",
            "Iteration: 570; Percent complete: 14.2%; Average loss: 5.0933\n",
            "Iteration: 580; Percent complete: 14.5%; Average loss: 4.9918\n",
            "Iteration: 590; Percent complete: 14.8%; Average loss: 5.0595\n",
            "Iteration: 600; Percent complete: 15.0%; Average loss: 4.9238\n",
            "Iteration: 610; Percent complete: 15.2%; Average loss: 4.9927\n",
            "Iteration: 620; Percent complete: 15.5%; Average loss: 4.9351\n",
            "Iteration: 630; Percent complete: 15.8%; Average loss: 4.8924\n",
            "Iteration: 640; Percent complete: 16.0%; Average loss: 4.8448\n",
            "Iteration: 650; Percent complete: 16.2%; Average loss: 4.7574\n",
            "Iteration: 660; Percent complete: 16.5%; Average loss: 4.8169\n",
            "Iteration: 670; Percent complete: 16.8%; Average loss: 4.7590\n",
            "Iteration: 680; Percent complete: 17.0%; Average loss: 4.6845\n",
            "Iteration: 690; Percent complete: 17.2%; Average loss: 4.7027\n",
            "Iteration: 700; Percent complete: 17.5%; Average loss: 4.6912\n",
            "Iteration: 710; Percent complete: 17.8%; Average loss: 4.7036\n",
            "Iteration: 720; Percent complete: 18.0%; Average loss: 4.7226\n",
            "Iteration: 730; Percent complete: 18.2%; Average loss: 4.5964\n",
            "Iteration: 740; Percent complete: 18.5%; Average loss: 4.4534\n",
            "Iteration: 750; Percent complete: 18.8%; Average loss: 4.4474\n",
            "Iteration: 760; Percent complete: 19.0%; Average loss: 4.4642\n",
            "Iteration: 770; Percent complete: 19.2%; Average loss: 4.4307\n",
            "Iteration: 780; Percent complete: 19.5%; Average loss: 4.3371\n",
            "Iteration: 790; Percent complete: 19.8%; Average loss: 4.3785\n",
            "Iteration: 800; Percent complete: 20.0%; Average loss: 4.2265\n",
            "Iteration: 810; Percent complete: 20.2%; Average loss: 4.3291\n",
            "Iteration: 820; Percent complete: 20.5%; Average loss: 4.3470\n",
            "Iteration: 830; Percent complete: 20.8%; Average loss: 4.2849\n",
            "Iteration: 840; Percent complete: 21.0%; Average loss: 4.1949\n",
            "Iteration: 850; Percent complete: 21.2%; Average loss: 4.1272\n",
            "Iteration: 860; Percent complete: 21.5%; Average loss: 4.2303\n",
            "Iteration: 870; Percent complete: 21.8%; Average loss: 4.0977\n",
            "Iteration: 880; Percent complete: 22.0%; Average loss: 4.1716\n",
            "Iteration: 890; Percent complete: 22.2%; Average loss: 3.9897\n",
            "Iteration: 900; Percent complete: 22.5%; Average loss: 4.0345\n",
            "Iteration: 910; Percent complete: 22.8%; Average loss: 3.8916\n",
            "Iteration: 920; Percent complete: 23.0%; Average loss: 3.9628\n",
            "Iteration: 930; Percent complete: 23.2%; Average loss: 4.0159\n",
            "Iteration: 940; Percent complete: 23.5%; Average loss: 3.8916\n",
            "Iteration: 950; Percent complete: 23.8%; Average loss: 3.9318\n",
            "Iteration: 960; Percent complete: 24.0%; Average loss: 3.8421\n",
            "Iteration: 970; Percent complete: 24.2%; Average loss: 3.8676\n",
            "Iteration: 980; Percent complete: 24.5%; Average loss: 3.7310\n",
            "Iteration: 990; Percent complete: 24.8%; Average loss: 3.7359\n",
            "Iteration: 1000; Percent complete: 25.0%; Average loss: 3.6708\n",
            "Iteration: 1010; Percent complete: 25.2%; Average loss: 3.7080\n",
            "Iteration: 1020; Percent complete: 25.5%; Average loss: 3.6967\n",
            "Iteration: 1030; Percent complete: 25.8%; Average loss: 3.5859\n",
            "Iteration: 1040; Percent complete: 26.0%; Average loss: 3.5224\n",
            "Iteration: 1050; Percent complete: 26.2%; Average loss: 3.6452\n",
            "Iteration: 1060; Percent complete: 26.5%; Average loss: 3.5365\n",
            "Iteration: 1070; Percent complete: 26.8%; Average loss: 3.4706\n",
            "Iteration: 1080; Percent complete: 27.0%; Average loss: 3.4888\n",
            "Iteration: 1090; Percent complete: 27.3%; Average loss: 3.4685\n",
            "Iteration: 1100; Percent complete: 27.5%; Average loss: 3.4223\n",
            "Iteration: 1110; Percent complete: 27.8%; Average loss: 3.4456\n",
            "Iteration: 1120; Percent complete: 28.0%; Average loss: 3.3249\n",
            "Iteration: 1130; Percent complete: 28.2%; Average loss: 3.3170\n",
            "Iteration: 1140; Percent complete: 28.5%; Average loss: 3.3943\n",
            "Iteration: 1150; Percent complete: 28.7%; Average loss: 3.2044\n",
            "Iteration: 1160; Percent complete: 29.0%; Average loss: 3.3225\n",
            "Iteration: 1170; Percent complete: 29.2%; Average loss: 3.1184\n",
            "Iteration: 1180; Percent complete: 29.5%; Average loss: 3.1928\n",
            "Iteration: 1190; Percent complete: 29.8%; Average loss: 3.1459\n",
            "Iteration: 1200; Percent complete: 30.0%; Average loss: 3.0963\n",
            "Iteration: 1210; Percent complete: 30.2%; Average loss: 3.2290\n",
            "Iteration: 1220; Percent complete: 30.5%; Average loss: 3.0369\n",
            "Iteration: 1230; Percent complete: 30.8%; Average loss: 3.1319\n",
            "Iteration: 1240; Percent complete: 31.0%; Average loss: 3.1035\n",
            "Iteration: 1250; Percent complete: 31.2%; Average loss: 3.0152\n",
            "Iteration: 1260; Percent complete: 31.5%; Average loss: 3.0002\n",
            "Iteration: 1270; Percent complete: 31.8%; Average loss: 3.0242\n",
            "Iteration: 1280; Percent complete: 32.0%; Average loss: 2.9988\n",
            "Iteration: 1290; Percent complete: 32.2%; Average loss: 2.9401\n",
            "Iteration: 1300; Percent complete: 32.5%; Average loss: 2.9367\n",
            "Iteration: 1310; Percent complete: 32.8%; Average loss: 2.8245\n",
            "Iteration: 1320; Percent complete: 33.0%; Average loss: 2.8110\n",
            "Iteration: 1330; Percent complete: 33.2%; Average loss: 2.7705\n",
            "Iteration: 1340; Percent complete: 33.5%; Average loss: 2.8509\n",
            "Iteration: 1350; Percent complete: 33.8%; Average loss: 2.7995\n",
            "Iteration: 1360; Percent complete: 34.0%; Average loss: 2.8306\n",
            "Iteration: 1370; Percent complete: 34.2%; Average loss: 2.8185\n",
            "Iteration: 1380; Percent complete: 34.5%; Average loss: 2.6692\n",
            "Iteration: 1390; Percent complete: 34.8%; Average loss: 2.6732\n",
            "Iteration: 1400; Percent complete: 35.0%; Average loss: 2.7169\n",
            "Iteration: 1410; Percent complete: 35.2%; Average loss: 2.6085\n",
            "Iteration: 1420; Percent complete: 35.5%; Average loss: 2.6362\n",
            "Iteration: 1430; Percent complete: 35.8%; Average loss: 2.5323\n",
            "Iteration: 1440; Percent complete: 36.0%; Average loss: 2.5786\n",
            "Iteration: 1450; Percent complete: 36.2%; Average loss: 2.5985\n",
            "Iteration: 1460; Percent complete: 36.5%; Average loss: 2.5970\n",
            "Iteration: 1470; Percent complete: 36.8%; Average loss: 2.4685\n",
            "Iteration: 1480; Percent complete: 37.0%; Average loss: 2.5478\n",
            "Iteration: 1490; Percent complete: 37.2%; Average loss: 2.4642\n",
            "Iteration: 1500; Percent complete: 37.5%; Average loss: 2.4879\n",
            "Iteration: 1510; Percent complete: 37.8%; Average loss: 2.4536\n",
            "Iteration: 1520; Percent complete: 38.0%; Average loss: 2.4021\n",
            "Iteration: 1530; Percent complete: 38.2%; Average loss: 2.3882\n",
            "Iteration: 1540; Percent complete: 38.5%; Average loss: 2.3833\n",
            "Iteration: 1550; Percent complete: 38.8%; Average loss: 2.3845\n",
            "Iteration: 1560; Percent complete: 39.0%; Average loss: 2.3908\n",
            "Iteration: 1570; Percent complete: 39.2%; Average loss: 2.2770\n",
            "Iteration: 1580; Percent complete: 39.5%; Average loss: 2.3163\n",
            "Iteration: 1590; Percent complete: 39.8%; Average loss: 2.2499\n",
            "Iteration: 1600; Percent complete: 40.0%; Average loss: 2.3071\n",
            "Iteration: 1610; Percent complete: 40.2%; Average loss: 2.3085\n",
            "Iteration: 1620; Percent complete: 40.5%; Average loss: 2.1698\n",
            "Iteration: 1630; Percent complete: 40.8%; Average loss: 2.1907\n",
            "Iteration: 1640; Percent complete: 41.0%; Average loss: 2.1662\n",
            "Iteration: 1650; Percent complete: 41.2%; Average loss: 2.1130\n",
            "Iteration: 1660; Percent complete: 41.5%; Average loss: 2.0796\n",
            "Iteration: 1670; Percent complete: 41.8%; Average loss: 2.0620\n",
            "Iteration: 1680; Percent complete: 42.0%; Average loss: 2.1354\n",
            "Iteration: 1690; Percent complete: 42.2%; Average loss: 2.0037\n",
            "Iteration: 1700; Percent complete: 42.5%; Average loss: 2.0378\n",
            "Iteration: 1710; Percent complete: 42.8%; Average loss: 1.9636\n",
            "Iteration: 1720; Percent complete: 43.0%; Average loss: 1.9151\n",
            "Iteration: 1730; Percent complete: 43.2%; Average loss: 1.9616\n",
            "Iteration: 1740; Percent complete: 43.5%; Average loss: 2.0497\n",
            "Iteration: 1750; Percent complete: 43.8%; Average loss: 1.9836\n",
            "Iteration: 1760; Percent complete: 44.0%; Average loss: 1.9978\n",
            "Iteration: 1770; Percent complete: 44.2%; Average loss: 1.9818\n",
            "Iteration: 1780; Percent complete: 44.5%; Average loss: 1.9116\n",
            "Iteration: 1790; Percent complete: 44.8%; Average loss: 1.8765\n",
            "Iteration: 1800; Percent complete: 45.0%; Average loss: 1.8824\n",
            "Iteration: 1810; Percent complete: 45.2%; Average loss: 1.9503\n",
            "Iteration: 1820; Percent complete: 45.5%; Average loss: 1.8485\n",
            "Iteration: 1830; Percent complete: 45.8%; Average loss: 1.9237\n",
            "Iteration: 1840; Percent complete: 46.0%; Average loss: 1.8226\n",
            "Iteration: 1850; Percent complete: 46.2%; Average loss: 1.8220\n",
            "Iteration: 1860; Percent complete: 46.5%; Average loss: 1.8108\n",
            "Iteration: 1870; Percent complete: 46.8%; Average loss: 1.7810\n",
            "Iteration: 1880; Percent complete: 47.0%; Average loss: 1.7905\n",
            "Iteration: 1890; Percent complete: 47.2%; Average loss: 1.7787\n",
            "Iteration: 1900; Percent complete: 47.5%; Average loss: 1.7714\n",
            "Iteration: 1910; Percent complete: 47.8%; Average loss: 1.6544\n",
            "Iteration: 1920; Percent complete: 48.0%; Average loss: 1.7444\n",
            "Iteration: 1930; Percent complete: 48.2%; Average loss: 1.7566\n",
            "Iteration: 1940; Percent complete: 48.5%; Average loss: 1.6048\n",
            "Iteration: 1950; Percent complete: 48.8%; Average loss: 1.6089\n",
            "Iteration: 1960; Percent complete: 49.0%; Average loss: 1.6113\n",
            "Iteration: 1970; Percent complete: 49.2%; Average loss: 1.6655\n",
            "Iteration: 1980; Percent complete: 49.5%; Average loss: 1.6128\n",
            "Iteration: 1990; Percent complete: 49.8%; Average loss: 1.5472\n",
            "Iteration: 2000; Percent complete: 50.0%; Average loss: 1.6283\n",
            "Iteration: 2010; Percent complete: 50.2%; Average loss: 1.5444\n",
            "Iteration: 2020; Percent complete: 50.5%; Average loss: 1.5328\n",
            "Iteration: 2030; Percent complete: 50.7%; Average loss: 1.5628\n",
            "Iteration: 2040; Percent complete: 51.0%; Average loss: 1.6000\n",
            "Iteration: 2050; Percent complete: 51.2%; Average loss: 1.5538\n",
            "Iteration: 2060; Percent complete: 51.5%; Average loss: 1.4544\n",
            "Iteration: 2070; Percent complete: 51.7%; Average loss: 1.4940\n",
            "Iteration: 2080; Percent complete: 52.0%; Average loss: 1.4698\n",
            "Iteration: 2090; Percent complete: 52.2%; Average loss: 1.4667\n",
            "Iteration: 2100; Percent complete: 52.5%; Average loss: 1.4581\n",
            "Iteration: 2110; Percent complete: 52.8%; Average loss: 1.4914\n",
            "Iteration: 2120; Percent complete: 53.0%; Average loss: 1.3936\n",
            "Iteration: 2130; Percent complete: 53.2%; Average loss: 1.4355\n",
            "Iteration: 2140; Percent complete: 53.5%; Average loss: 1.4148\n",
            "Iteration: 2150; Percent complete: 53.8%; Average loss: 1.4216\n",
            "Iteration: 2160; Percent complete: 54.0%; Average loss: 1.4266\n",
            "Iteration: 2170; Percent complete: 54.2%; Average loss: 1.3618\n",
            "Iteration: 2180; Percent complete: 54.5%; Average loss: 1.3534\n",
            "Iteration: 2190; Percent complete: 54.8%; Average loss: 1.3438\n",
            "Iteration: 2200; Percent complete: 55.0%; Average loss: 1.3420\n",
            "Iteration: 2210; Percent complete: 55.2%; Average loss: 1.3088\n",
            "Iteration: 2220; Percent complete: 55.5%; Average loss: 1.3298\n",
            "Iteration: 2230; Percent complete: 55.8%; Average loss: 1.3687\n",
            "Iteration: 2240; Percent complete: 56.0%; Average loss: 1.2953\n",
            "Iteration: 2250; Percent complete: 56.2%; Average loss: 1.2619\n",
            "Iteration: 2260; Percent complete: 56.5%; Average loss: 1.3042\n",
            "Iteration: 2270; Percent complete: 56.8%; Average loss: 1.2852\n",
            "Iteration: 2280; Percent complete: 57.0%; Average loss: 1.2022\n",
            "Iteration: 2290; Percent complete: 57.2%; Average loss: 1.1971\n",
            "Iteration: 2300; Percent complete: 57.5%; Average loss: 1.1778\n",
            "Iteration: 2310; Percent complete: 57.8%; Average loss: 1.1367\n",
            "Iteration: 2320; Percent complete: 58.0%; Average loss: 1.1588\n",
            "Iteration: 2330; Percent complete: 58.2%; Average loss: 1.1982\n",
            "Iteration: 2340; Percent complete: 58.5%; Average loss: 1.2139\n",
            "Iteration: 2350; Percent complete: 58.8%; Average loss: 1.1880\n",
            "Iteration: 2360; Percent complete: 59.0%; Average loss: 1.2059\n",
            "Iteration: 2370; Percent complete: 59.2%; Average loss: 1.1566\n",
            "Iteration: 2380; Percent complete: 59.5%; Average loss: 1.1941\n",
            "Iteration: 2390; Percent complete: 59.8%; Average loss: 1.1456\n",
            "Iteration: 2400; Percent complete: 60.0%; Average loss: 1.1561\n",
            "Iteration: 2410; Percent complete: 60.2%; Average loss: 1.1284\n",
            "Iteration: 2420; Percent complete: 60.5%; Average loss: 1.1152\n",
            "Iteration: 2430; Percent complete: 60.8%; Average loss: 1.1170\n",
            "Iteration: 2440; Percent complete: 61.0%; Average loss: 1.1296\n",
            "Iteration: 2450; Percent complete: 61.3%; Average loss: 1.0563\n",
            "Iteration: 2460; Percent complete: 61.5%; Average loss: 1.1371\n",
            "Iteration: 2470; Percent complete: 61.8%; Average loss: 1.0106\n",
            "Iteration: 2480; Percent complete: 62.0%; Average loss: 1.0540\n",
            "Iteration: 2490; Percent complete: 62.3%; Average loss: 1.0589\n",
            "Iteration: 2500; Percent complete: 62.5%; Average loss: 1.0757\n",
            "Iteration: 2510; Percent complete: 62.7%; Average loss: 1.0069\n",
            "Iteration: 2520; Percent complete: 63.0%; Average loss: 0.9763\n",
            "Iteration: 2530; Percent complete: 63.2%; Average loss: 1.0117\n",
            "Iteration: 2540; Percent complete: 63.5%; Average loss: 0.9945\n",
            "Iteration: 2550; Percent complete: 63.7%; Average loss: 0.9797\n",
            "Iteration: 2560; Percent complete: 64.0%; Average loss: 0.9534\n",
            "Iteration: 2570; Percent complete: 64.2%; Average loss: 0.9908\n",
            "Iteration: 2580; Percent complete: 64.5%; Average loss: 0.9876\n",
            "Iteration: 2590; Percent complete: 64.8%; Average loss: 1.0117\n",
            "Iteration: 2600; Percent complete: 65.0%; Average loss: 1.0030\n",
            "Iteration: 2610; Percent complete: 65.2%; Average loss: 0.9480\n",
            "Iteration: 2620; Percent complete: 65.5%; Average loss: 0.9624\n",
            "Iteration: 2630; Percent complete: 65.8%; Average loss: 0.9540\n",
            "Iteration: 2640; Percent complete: 66.0%; Average loss: 0.8993\n",
            "Iteration: 2650; Percent complete: 66.2%; Average loss: 0.9157\n",
            "Iteration: 2660; Percent complete: 66.5%; Average loss: 0.9388\n",
            "Iteration: 2670; Percent complete: 66.8%; Average loss: 0.9325\n",
            "Iteration: 2680; Percent complete: 67.0%; Average loss: 0.8820\n",
            "Iteration: 2690; Percent complete: 67.2%; Average loss: 0.8906\n",
            "Iteration: 2700; Percent complete: 67.5%; Average loss: 0.8746\n",
            "Iteration: 2710; Percent complete: 67.8%; Average loss: 0.8963\n",
            "Iteration: 2720; Percent complete: 68.0%; Average loss: 0.9358\n",
            "Iteration: 2730; Percent complete: 68.2%; Average loss: 0.8875\n",
            "Iteration: 2740; Percent complete: 68.5%; Average loss: 0.8463\n",
            "Iteration: 2750; Percent complete: 68.8%; Average loss: 0.8323\n",
            "Iteration: 2760; Percent complete: 69.0%; Average loss: 0.8117\n",
            "Iteration: 2770; Percent complete: 69.2%; Average loss: 0.8812\n",
            "Iteration: 2780; Percent complete: 69.5%; Average loss: 0.8445\n",
            "Iteration: 2790; Percent complete: 69.8%; Average loss: 0.8074\n",
            "Iteration: 2800; Percent complete: 70.0%; Average loss: 0.7770\n",
            "Iteration: 2810; Percent complete: 70.2%; Average loss: 0.8198\n",
            "Iteration: 2820; Percent complete: 70.5%; Average loss: 0.8169\n",
            "Iteration: 2830; Percent complete: 70.8%; Average loss: 0.8067\n",
            "Iteration: 2840; Percent complete: 71.0%; Average loss: 0.8349\n",
            "Iteration: 2850; Percent complete: 71.2%; Average loss: 0.8284\n",
            "Iteration: 2860; Percent complete: 71.5%; Average loss: 0.7930\n",
            "Iteration: 2870; Percent complete: 71.8%; Average loss: 0.7909\n",
            "Iteration: 2880; Percent complete: 72.0%; Average loss: 0.7701\n",
            "Iteration: 2890; Percent complete: 72.2%; Average loss: 0.8040\n",
            "Iteration: 2900; Percent complete: 72.5%; Average loss: 0.7929\n",
            "Iteration: 2910; Percent complete: 72.8%; Average loss: 0.7652\n",
            "Iteration: 2920; Percent complete: 73.0%; Average loss: 0.7462\n",
            "Iteration: 2930; Percent complete: 73.2%; Average loss: 0.7445\n",
            "Iteration: 2940; Percent complete: 73.5%; Average loss: 0.7551\n",
            "Iteration: 2950; Percent complete: 73.8%; Average loss: 0.7302\n",
            "Iteration: 2960; Percent complete: 74.0%; Average loss: 0.7403\n",
            "Iteration: 2970; Percent complete: 74.2%; Average loss: 0.7311\n",
            "Iteration: 2980; Percent complete: 74.5%; Average loss: 0.7109\n",
            "Iteration: 2990; Percent complete: 74.8%; Average loss: 0.6980\n",
            "Iteration: 3000; Percent complete: 75.0%; Average loss: 0.7336\n",
            "Iteration: 3010; Percent complete: 75.2%; Average loss: 0.6719\n",
            "Iteration: 3020; Percent complete: 75.5%; Average loss: 0.6752\n",
            "Iteration: 3030; Percent complete: 75.8%; Average loss: 0.6795\n",
            "Iteration: 3040; Percent complete: 76.0%; Average loss: 0.7129\n",
            "Iteration: 3050; Percent complete: 76.2%; Average loss: 0.6754\n",
            "Iteration: 3060; Percent complete: 76.5%; Average loss: 0.6994\n",
            "Iteration: 3070; Percent complete: 76.8%; Average loss: 0.6672\n",
            "Iteration: 3080; Percent complete: 77.0%; Average loss: 0.6732\n",
            "Iteration: 3090; Percent complete: 77.2%; Average loss: 0.6528\n",
            "Iteration: 3100; Percent complete: 77.5%; Average loss: 0.6712\n",
            "Iteration: 3110; Percent complete: 77.8%; Average loss: 0.6764\n",
            "Iteration: 3120; Percent complete: 78.0%; Average loss: 0.6954\n",
            "Iteration: 3130; Percent complete: 78.2%; Average loss: 0.6367\n",
            "Iteration: 3140; Percent complete: 78.5%; Average loss: 0.6588\n",
            "Iteration: 3150; Percent complete: 78.8%; Average loss: 0.6336\n",
            "Iteration: 3160; Percent complete: 79.0%; Average loss: 0.6314\n",
            "Iteration: 3170; Percent complete: 79.2%; Average loss: 0.6254\n",
            "Iteration: 3180; Percent complete: 79.5%; Average loss: 0.6507\n",
            "Iteration: 3190; Percent complete: 79.8%; Average loss: 0.5978\n",
            "Iteration: 3200; Percent complete: 80.0%; Average loss: 0.5810\n",
            "Iteration: 3210; Percent complete: 80.2%; Average loss: 0.6321\n",
            "Iteration: 3220; Percent complete: 80.5%; Average loss: 0.6465\n",
            "Iteration: 3230; Percent complete: 80.8%; Average loss: 0.5964\n",
            "Iteration: 3240; Percent complete: 81.0%; Average loss: 0.6149\n",
            "Iteration: 3250; Percent complete: 81.2%; Average loss: 0.6312\n",
            "Iteration: 3260; Percent complete: 81.5%; Average loss: 0.5986\n",
            "Iteration: 3270; Percent complete: 81.8%; Average loss: 0.5749\n",
            "Iteration: 3280; Percent complete: 82.0%; Average loss: 0.6088\n",
            "Iteration: 3290; Percent complete: 82.2%; Average loss: 0.5923\n",
            "Iteration: 3300; Percent complete: 82.5%; Average loss: 0.6084\n",
            "Iteration: 3310; Percent complete: 82.8%; Average loss: 0.5963\n",
            "Iteration: 3320; Percent complete: 83.0%; Average loss: 0.5599\n",
            "Iteration: 3330; Percent complete: 83.2%; Average loss: 0.5642\n",
            "Iteration: 3340; Percent complete: 83.5%; Average loss: 0.6060\n",
            "Iteration: 3350; Percent complete: 83.8%; Average loss: 0.5607\n",
            "Iteration: 3360; Percent complete: 84.0%; Average loss: 0.5842\n",
            "Iteration: 3370; Percent complete: 84.2%; Average loss: 0.5347\n",
            "Iteration: 3380; Percent complete: 84.5%; Average loss: 0.5460\n",
            "Iteration: 3390; Percent complete: 84.8%; Average loss: 0.5379\n",
            "Iteration: 3400; Percent complete: 85.0%; Average loss: 0.5528\n",
            "Iteration: 3410; Percent complete: 85.2%; Average loss: 0.5372\n",
            "Iteration: 3420; Percent complete: 85.5%; Average loss: 0.5650\n",
            "Iteration: 3430; Percent complete: 85.8%; Average loss: 0.5741\n",
            "Iteration: 3440; Percent complete: 86.0%; Average loss: 0.5236\n",
            "Iteration: 3450; Percent complete: 86.2%; Average loss: 0.5477\n",
            "Iteration: 3460; Percent complete: 86.5%; Average loss: 0.5211\n",
            "Iteration: 3470; Percent complete: 86.8%; Average loss: 0.5266\n",
            "Iteration: 3480; Percent complete: 87.0%; Average loss: 0.5173\n",
            "Iteration: 3490; Percent complete: 87.2%; Average loss: 0.5059\n",
            "Iteration: 3500; Percent complete: 87.5%; Average loss: 0.5310\n",
            "Iteration: 3510; Percent complete: 87.8%; Average loss: 0.4917\n",
            "Iteration: 3520; Percent complete: 88.0%; Average loss: 0.4971\n",
            "Iteration: 3530; Percent complete: 88.2%; Average loss: 0.5452\n",
            "Iteration: 3540; Percent complete: 88.5%; Average loss: 0.5296\n",
            "Iteration: 3550; Percent complete: 88.8%; Average loss: 0.5079\n",
            "Iteration: 3560; Percent complete: 89.0%; Average loss: 0.5300\n",
            "Iteration: 3570; Percent complete: 89.2%; Average loss: 0.4836\n",
            "Iteration: 3580; Percent complete: 89.5%; Average loss: 0.4919\n",
            "Iteration: 3590; Percent complete: 89.8%; Average loss: 0.4904\n",
            "Iteration: 3600; Percent complete: 90.0%; Average loss: 0.5052\n",
            "Iteration: 3610; Percent complete: 90.2%; Average loss: 0.5030\n",
            "Iteration: 3620; Percent complete: 90.5%; Average loss: 0.4987\n",
            "Iteration: 3630; Percent complete: 90.8%; Average loss: 0.4942\n",
            "Iteration: 3640; Percent complete: 91.0%; Average loss: 0.5044\n",
            "Iteration: 3650; Percent complete: 91.2%; Average loss: 0.4731\n",
            "Iteration: 3660; Percent complete: 91.5%; Average loss: 0.5090\n",
            "Iteration: 3670; Percent complete: 91.8%; Average loss: 0.4939\n",
            "Iteration: 3680; Percent complete: 92.0%; Average loss: 0.4722\n",
            "Iteration: 3690; Percent complete: 92.2%; Average loss: 0.4722\n",
            "Iteration: 3700; Percent complete: 92.5%; Average loss: 0.4741\n",
            "Iteration: 3710; Percent complete: 92.8%; Average loss: 0.5035\n",
            "Iteration: 3720; Percent complete: 93.0%; Average loss: 0.4585\n",
            "Iteration: 3730; Percent complete: 93.2%; Average loss: 0.4849\n",
            "Iteration: 3740; Percent complete: 93.5%; Average loss: 0.4869\n",
            "Iteration: 3750; Percent complete: 93.8%; Average loss: 0.4844\n",
            "Iteration: 3760; Percent complete: 94.0%; Average loss: 0.4601\n",
            "Iteration: 3770; Percent complete: 94.2%; Average loss: 0.4613\n",
            "Iteration: 3780; Percent complete: 94.5%; Average loss: 0.4673\n",
            "Iteration: 3790; Percent complete: 94.8%; Average loss: 0.4494\n",
            "Iteration: 3800; Percent complete: 95.0%; Average loss: 0.4468\n",
            "Iteration: 3810; Percent complete: 95.2%; Average loss: 0.4686\n",
            "Iteration: 3820; Percent complete: 95.5%; Average loss: 0.4833\n",
            "Iteration: 3830; Percent complete: 95.8%; Average loss: 0.4629\n",
            "Iteration: 3840; Percent complete: 96.0%; Average loss: 0.4510\n",
            "Iteration: 3850; Percent complete: 96.2%; Average loss: 0.4113\n",
            "Iteration: 3860; Percent complete: 96.5%; Average loss: 0.4324\n",
            "Iteration: 3870; Percent complete: 96.8%; Average loss: 0.4497\n",
            "Iteration: 3880; Percent complete: 97.0%; Average loss: 0.4636\n",
            "Iteration: 3890; Percent complete: 97.2%; Average loss: 0.4260\n",
            "Iteration: 3900; Percent complete: 97.5%; Average loss: 0.4451\n",
            "Iteration: 3910; Percent complete: 97.8%; Average loss: 0.4313\n",
            "Iteration: 3920; Percent complete: 98.0%; Average loss: 0.4321\n",
            "Iteration: 3930; Percent complete: 98.2%; Average loss: 0.4382\n",
            "Iteration: 3940; Percent complete: 98.5%; Average loss: 0.4264\n",
            "Iteration: 3950; Percent complete: 98.8%; Average loss: 0.4491\n",
            "Iteration: 3960; Percent complete: 99.0%; Average loss: 0.4404\n",
            "Iteration: 3970; Percent complete: 99.2%; Average loss: 0.4359\n",
            "Iteration: 3980; Percent complete: 99.5%; Average loss: 0.4481\n",
            "Iteration: 3990; Percent complete: 99.8%; Average loss: 0.4222\n",
            "Iteration: 4000; Percent complete: 100.0%; Average loss: 0.4269\n"
          ]
        }
      ],
      "source": [
        "# Configure training/optimization\n",
        "clip = TRAIN_CONFIG['clip']\n",
        "teacher_forcing_ratio = TRAIN_CONFIG['teacher_forcing_ratio']\n",
        "learning_rate = TRAIN_CONFIG['learning_rate']\n",
        "decoder_learning_ratio = TRAIN_CONFIG['decoder_learning_ratio']\n",
        "n_iteration = TRAIN_CONFIG['n_iteration']\n",
        "print_every = TRAIN_CONFIG['print_every']\n",
        "save_every = TRAIN_CONFIG['save_every']\n",
        "\n",
        "# Ensure dropout layers are in train mode\n",
        "encoder.train()\n",
        "decoder.train()\n",
        "\n",
        "# Initialize optimizers\n",
        "print('Building optimizers ...')\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
        "if loadFilename:\n",
        "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
        "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
        "\n",
        "# If you have cuda, configure cuda to call\n",
        "for state in encoder_optimizer.state.values():\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            state[k] = v.cuda()\n",
        "\n",
        "for state in decoder_optimizer.state.values():\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            state[k] = v.cuda()\n",
        "    \n",
        "# Run training iterations\n",
        "print(\"Starting Training!\")\n",
        "trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
        "           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
        "           print_every, save_every, clip, corpus_name, loadFilename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQ2M_7F9Dv1j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "outputId": "534bc11e-bdad-4ac1-c277-681ac918da0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> امروز چیکار کردی ؟\n",
            "<class 'torch.Tensor'>\n",
            "Bot: هیچی همش خواب بودم و فیلم دیدم که خیالت راحت بشه یا حداقل داره ولی تو جمع میشه کار میکنه تو زندگی شده\n",
            "> خدا حافظ\n",
            "Error: Encountered unknown word.\n",
            "> خداحافظ\n",
            "Error: Encountered unknown word.\n",
            "> خدافظ\n",
            "<class 'torch.Tensor'>\n",
            "Bot: به سلامت کاکو که میتونی تو لیاقت خوب بشه که میشی زود که بهت باشم تا ساعت اثر داره که از سالگی سالی زندگی شد یه\n",
            "> بهش چی بگم ؟\n",
            "<class 'torch.Tensor'>\n",
            "Bot: هیچی ندادم تا سال دیگه آهن جدا که میتونی بره چون خونه تا دختر بود نه که خیالت راحت بشه میتونی ازش بگیر\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-3de5b829fbba>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Begin chatting (uncomment and run the following line to begin)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mevaluateInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-55-6889d915e2b2>\u001b[0m in \u001b[0;36mevaluateInput\u001b[0;34m(encoder, decoder, searcher, voc)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m# Get input sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0minput_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'> '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0;31m# Check if it is quit case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minput_sentence\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'q'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minput_sentence\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'quit'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "# Set dropout layers to eval mode\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "\n",
        "# Initialize search module\n",
        "searcher = GreedySearchDecoder(encoder, decoder)\n",
        "\n",
        "# Begin chatting (uncomment and run the following line to begin)\n",
        "evaluateInput(encoder, decoder, searcher, voc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3KO_QQjmhcC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55baaf30-8d61-4491-f7c0-002b7a432fae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['فردا واکسن ۱۸ ماهگی پسرمه چه کارهای بکنم زود خوب بشه از استرسش خوابم نمیاد',\n",
              " 'قبل رفتن استامینوفن بده کمپرس هم طبق چیزی ک میگن و اینکه حتی تب نداره استامینوفن بده ک درد اذیتش نکنه']"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "pairs[1101]"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8hcUNTt_d2Ez"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}